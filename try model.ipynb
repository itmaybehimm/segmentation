{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe551786-cc7e-498f-a626-7cefd9649c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers,models\n",
    "from keras.applications import VGG19\n",
    "from matplotlib import pyplot as plt\n",
    "from keras import backend as K\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "340f1301-549d-493c-81aa-d516c2316f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80134624/80134624 [==============================] - 6s 0us/step\n"
     ]
    }
   ],
   "source": [
    "def vgg16_feature_extractor(input_shape):\n",
    "    # Create VGG16 base model\n",
    "    base_model = VGG19(include_top=False, input_shape=input_shape, weights=\"imagenet\")\n",
    "\n",
    "    # Freeze the layers of the base model\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Get the output of the base model\n",
    "    output = base_model.output\n",
    "\n",
    "    # Flatten the output feature vectors\n",
    "    output = layers.GlobalAveragePooling2D()(output)\n",
    "\n",
    "    # Create the model\n",
    "    model = models.Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_siamese_vgg16(input_shape):\n",
    "    # Define the input layer for the first image\n",
    "    input_a = layers.Input(shape=input_shape, name=\"input_a\")\n",
    "    # Define the input layer for the second image\n",
    "    input_b = layers.Input(shape=input_shape, name=\"input_b\")\n",
    "\n",
    "    # data_augmentation = tf.keras.Sequential([\n",
    "    #   layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    #   layers.RandomRotation(0.2),\n",
    "    # ], name = \"data_augmentation\")\n",
    "    # augmented_input_a = data_augmentation(input_a)\n",
    "    # augmented_input_b = data_augmentation(input_b)\n",
    "    # Define the VGG16 model (excluding the top layers)\n",
    "    base_model = vgg16_feature_extractor(input_shape)\n",
    "\n",
    "    # Get the output feature vectors from the base model for both inputs\n",
    "    output_a = base_model(input_a)\n",
    "    output_b = base_model(input_b)\n",
    "\n",
    "    # output_a = Dropout(0.2)(output_a)\n",
    "    # output_b = Dropout(0.2)(output_b)\n",
    "    # concatenated_features = layers.Concatenate()([output_a, output_b])\n",
    "\n",
    "    # Distance calculation\n",
    "    distance = layers.Lambda(lambda x: K.abs(x[0] - x[1]), name=\"distance\")(\n",
    "        [output_a, output_b]\n",
    "    )\n",
    "\n",
    "    # Output layer\n",
    "    output = layers.Dense(1, activation=\"sigmoid\", name=\"output\")(distance)\n",
    "\n",
    "    # Create the Siamese model\n",
    "    siamese_model = models.Model(\n",
    "        inputs=[input_a, input_b], outputs=output, name=\"siamese_vgg16\"\n",
    "    )\n",
    "\n",
    "    return siamese_model\n",
    "\n",
    "# Set the input shape for the VGG16 model\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "# Build the Siamese VGG16 twins model\n",
    "\n",
    "model = build_siamese_vgg16(input_shape)\n",
    "\n",
    "# Compile the model with contrastive loss\n",
    "\n",
    "\n",
    "# Display the model summary\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0935c46c-4aa5-4042-905f-d5d3812c8360",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer =tf.keras.optimizers.legacy.Adam(learning_rate=0.01),\n",
    "                        loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                        metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39e6bf3c-25d1-4049-8a9b-6b633da4dbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"siamese_vgg16\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_a (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " input_b (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " model_1 (Functional)        (None, 512)                  2002438   ['input_a[0][0]',             \n",
      "                                                          4          'input_b[0][0]']             \n",
      "                                                                                                  \n",
      " distance (Lambda)           (None, 512)                  0         ['model_1[0][0]',             \n",
      "                                                                     'model_1[1][0]']             \n",
      "                                                                                                  \n",
      " output (Dense)              (None, 1)                    513       ['distance[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 20024897 (76.39 MB)\n",
      "Trainable params: 513 (2.00 KB)\n",
      "Non-trainable params: 20024384 (76.39 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model = tf.keras.models.load_model('current.keras',safe_mode=False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "599d9ecd-757d-46a0-b727-780cda17ff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path=os.path.join(\"mejan\",\"chalde_plz_FINE19_1.h5\")\n",
    "model.load_weights(\"gedu.h5\")\n",
    "# model.save_weights(\"gedu.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fbe963fa-5b9a-4f83-80ff-dac19a93118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    print(type(image))\n",
    "    \n",
    "    image = tf.image.decode_bmp(image, channels=0)\n",
    "    # image=tf.image.grayscale_to_rgb(image)\n",
    "\n",
    "    image = tf.image.resize(image, size = (224,224))\n",
    "    image = image/255.0 \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3241252-fa24-4730-978b-6e1fb6457a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1=os.path.join(\"all-images-iit-normalized-2\",\"001_01_L.bmp\")\n",
    "img2=os.path.join(\"all-images-iit-normalized-2\",\"001_02_L.bmp\")\n",
    "img3=os.path.join(\"all-images-iit-normalized-2\",\"003_07_L.bmp\")\n",
    "img4=os.path.join(\"all-images-iit-normalized-2\",\"003_03_L.bmp\")\n",
    "img5=os.path.join(\"all-images-iit-normalized-2\",\"029_08_R.bmp\")\n",
    "img6=os.path.join(\"all-images-iit-normalized-2\",\"041_07_R.bmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c88e006-4d05-4863-84c6-54f51edfc81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([224, 224, 3]),\n",
       " TensorShape([224, 224, 3]),\n",
       " TensorShape([224, 224, 3]),\n",
       " TensorShape([224, 224, 3]),\n",
       " TensorShape([224, 224, 3]),\n",
       " TensorShape([224, 224, 3]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1=load_and_preprocess_image(img1)\n",
    "img2=load_and_preprocess_image(img2)\n",
    "img3=load_and_preprocess_image(img3)\n",
    "img4=load_and_preprocess_image(img4)\n",
    "img5=load_and_preprocess_image(img5)\n",
    "img6=load_and_preprocess_image(img6)\n",
    "\n",
    "img1.shape,img2.shape,img3.shape,img4.shape,img5.shape,img6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18e849a4-38c3-43e0-8142-f81f31c252f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(2,3, figsize=(8, 8))\n",
    "\n",
    "# # Plot data on each subplot\n",
    "# axs[0, 0].imshow(tf.cast(img1, dtype=tf.int32))\n",
    "# axs[0, 0].set_title('Image 1')\n",
    "\n",
    "# axs[0, 1].imshow(tf.cast(img2, dtype=tf.int32))\n",
    "# axs[0, 1].set_title('Image 2')\n",
    "\n",
    "# axs[0, 2].imshow(tf.cast(img3, dtype=tf.int32))\n",
    "# axs[0, 2].set_title('Image 3')\n",
    "\n",
    "# axs[1, 0].imshow(tf.cast(img4, dtype=tf.int32))\n",
    "# axs[1, 0].set_title('Image 4')\n",
    "\n",
    "# axs[1, 1].imshow(tf.cast(img5, dtype=tf.int32))\n",
    "# axs[1, 1].set_title('Image 5')\n",
    "\n",
    "# axs[1, 2].imshow(tf.cast(img6, dtype=tf.int32))\n",
    "# axs[1, 2].set_title('Image 6')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dcbe7d1c-64f1-4159-8867-43cfe5dafa74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3)\n",
      "(1, 224, 224, 3)\n",
      "(1, 224, 224, 3)\n",
      "(1, 224, 224, 3)\n",
      "(1, 224, 224, 3)\n",
      "(1, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# Assuming img1, img2, ..., img6 are your image tensors\n",
    "\n",
    "# Add an extra dimension at the beginning for each image\n",
    "img1 = tf.expand_dims(img1, axis=0)\n",
    "img2 = tf.expand_dims(img2, axis=0)\n",
    "img3 = tf.expand_dims(img3, axis=0)\n",
    "img4 = tf.expand_dims(img4, axis=0)\n",
    "img5 = tf.expand_dims(img5, axis=0)\n",
    "img6 = tf.expand_dims(img6, axis=0)\n",
    "\n",
    "# Print the shape of each image tensor after adding the extra dimension\n",
    "print(img1.shape)  # Output: (1, 244, 244, 3)\n",
    "print(img2.shape)  # Output: (1, 244, 244, 3)\n",
    "print(img3.shape)  # Output: (1, 244, 244, 3)\n",
    "print(img4.shape)  # Output: (1, 244, 244, 3)\n",
    "print(img5.shape)  # Output: (1, 244, 244, 3)\n",
    "print(img6.shape)  # Output: (1, 244, 244, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5838b929-1a27-4429-8796-81c87ba48814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 415ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.96027267]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([img1,img1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e49cbb43-6f96-4c70-b8ec-f6581c0f8344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 256ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.7407202]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([img1,img2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2652b352-64d4-4150-900a-69dcb7127199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 254ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00414096]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([img1,img3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8990a34b-4a5e-4d4e-8ed5-e6daba6f0634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 311ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00567866]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([img2,img3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d18facf0-ed3d-49f8-ac36-cf5b1743a04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 260ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5736945]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([img3,img4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce631fdb-9b4d-487f-bb9c-7282cd284428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 261ms/step\n",
      "1/1 [==============================] - 0s 266ms/step\n",
      "1/1 [==============================] - 0s 275ms/step\n",
      "1/1 [==============================] - 0s 275ms/step\n",
      "1/1 [==============================] - 0s 346ms/step\n",
      "1/1 [==============================] - 0s 465ms/step\n",
      "1/1 [==============================] - 0s 257ms/step\n",
      "1/1 [==============================] - 0s 250ms/step\n",
      "1/1 [==============================] - 0s 256ms/step\n",
      "1/1 [==============================] - 0s 254ms/step\n",
      "1/1 [==============================] - 0s 263ms/step\n",
      "1/1 [==============================] - 0s 332ms/step\n",
      "1/1 [==============================] - 0s 293ms/step\n",
      "1/1 [==============================] - 0s 245ms/step\n",
      "1/1 [==============================] - 0s 281ms/step\n",
      "Image 1: img1, Image 2: img2, Prediction:[[0.7407202]] : 1\n",
      "Image 1: img1, Image 2: img3, Prediction:[[0.00414096]] : 0\n",
      "Image 1: img1, Image 2: img4, Prediction:[[0.04583964]] : 0\n",
      "Image 1: img1, Image 2: img5, Prediction:[[0.04213378]] : 0\n",
      "Image 1: img1, Image 2: img6, Prediction:[[0.18375196]] : 0\n",
      "Image 1: img2, Image 2: img3, Prediction:[[0.00567866]] : 0\n",
      "Image 1: img2, Image 2: img4, Prediction:[[0.01886606]] : 0\n",
      "Image 1: img2, Image 2: img5, Prediction:[[0.02024704]] : 0\n",
      "Image 1: img2, Image 2: img6, Prediction:[[0.11852754]] : 0\n",
      "Image 1: img3, Image 2: img4, Prediction:[[0.5736945]] : 1\n",
      "Image 1: img3, Image 2: img5, Prediction:[[0.17853732]] : 0\n",
      "Image 1: img3, Image 2: img6, Prediction:[[0.12492856]] : 0\n",
      "Image 1: img4, Image 2: img5, Prediction:[[0.2650101]] : 0\n",
      "Image 1: img4, Image 2: img6, Prediction:[[0.43074572]] : 0\n",
      "Image 1: img5, Image 2: img6, Prediction:[[0.39783382]] : 0\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "# img1=os.path.join(\"Images\",\"h1.bmp\")\n",
    "# img2=os.path.join(\"Images\",\"i1.bmp\")\n",
    "# img3=os.path.join(\"Images\",\"e2.bmp\")\n",
    "# img4=os.path.join(\"Images\",\"d2.bmp\")\n",
    "# img5=os.path.join(\"Images\",\"d1.bmp\")\n",
    "# img6=os.path.join(\"Images\",\"h2.bmp\")\n",
    "# Assuming model is your TensorFlow model\n",
    "for i in range(6):\n",
    "    for j in range(i+1, 6):\n",
    "        img1_name = f'img{i+1}'\n",
    "        img2_name = f'img{j+1}'\n",
    "        img1_data = eval(img1_name)\n",
    "        img2_data = eval(img2_name)\n",
    "        \n",
    "        # Predict for the pair of images (img_i, img_j)\n",
    "        prediction = model.predict([img1_data, img2_data])\n",
    "        predictions.append((img1_name, img2_name, prediction))\n",
    "\n",
    "# Now predictions list contains tuples of image names and predictions for all combinations of images\n",
    "for img1_name, img2_name, prediction in predictions:\n",
    "    print(f\"Image 1: {img1_name}, Image 2: {img2_name}, Prediction:{prediction} : {0 if prediction< 0.5 else 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a69ee9-5707-4381-ae53-e6961e2e22fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310tf2",
   "language": "python",
   "name": "py310tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
